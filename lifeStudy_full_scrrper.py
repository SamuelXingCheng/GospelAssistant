import requests
from bs4 import BeautifulSoup
import json
import time
import re
import os


# æ ¹æ“šé¸æ“‡çš„ç‰ˆæœ¬æ±ºå®š BASE_URL
BASE_URL = "https://www.lsmchinese.org/lifestudy/"
INDEX_PAGE = BASE_URL + "index.html"  # ç”Ÿå‘½è®€ç¶“ç´¢å¼•é 

def sanitize_filename(name):
    """ç§»é™¤ä¸åˆæ³•çš„æª”åå­—å…ƒ"""
    return re.sub(r'[\/:*?"<>|]', '_', name)

def is_old_testament(book_name):
    old_testament_books = {
        "å‰µä¸–è¨˜", "å‡ºåŸƒåŠè¨˜", "åˆ©æœªè¨˜", "æ°‘æ•¸è¨˜", "ç”³å‘½è¨˜", "ç´„æ›¸äºè¨˜", "å£«å¸«è¨˜", "è·¯å¾—è¨˜", 
        "æ’’æ¯è€³è¨˜", "åˆ—ç‹ç´€", "æ­·ä»£å¿—", 
        "ä»¥æ–¯æ‹‰è¨˜", "å°¼å¸Œç±³è¨˜", "ä»¥æ–¯å¸–è¨˜", "ç´„ä¼¯è¨˜", "è©©ç¯‡", "ç®´è¨€", "å‚³é“æ›¸", "é›…æ­Œ", 
        "ä»¥è³½äºæ›¸", "è€¶åˆ©ç±³æ›¸", "è€¶åˆ©ç±³å“€æ­Œ", "ä»¥è¥¿çµæ›¸", "ä½†ä»¥ç†æ›¸", "ä½•è¥¿é˜¿æ›¸", 
        "ç´„ç¥æ›¸", "é˜¿æ‘©å¸æ›¸", "ä¿„å·´åº•äºæ›¸", "ç´„æ‹¿æ›¸", "å½Œè¿¦æ›¸", "é‚£é´»æ›¸", "å“ˆå·´è°·æ›¸", 
        "è¥¿ç•ªé›…æ›¸", "å“ˆè©²æ›¸", "æ’’è¿¦åˆ©äºæ›¸", "ç‘ªæ‹‰åŸºæ›¸"
    }
    return book_name in old_testament_books

def get_book_links():
    """æ“·å–æ‰€æœ‰æ›¸å·çš„åç¨±èˆ‡å°æ‡‰ç´¢å¼•é é€£çµ"""
    response = requests.get(INDEX_PAGE)
    soup = BeautifulSoup(response.text, 'html.parser')
    books = {}
    for td in soup.select('table.ls-index td a'):
        name = td.text.strip()
        link = BASE_URL + td['href'].strip()
        books[name] = link  # æ›¸å·åç¨±: ç´¢å¼•é ç¶²å€
    return books

def get_chapter_links(book_name, book_url):
    """æ“·å–å–®æœ¬æ›¸çš„æ‰€æœ‰ç« ç¯€é€£çµ"""
    response = requests.get(book_url)
    soup = BeautifulSoup(response.text, 'html.parser')
    chapters = {}
    for a in soup.select('div.ls-text p a'):  # ç« ç¯€ç´¢å¼•å…§çš„é€£çµ
        title = a.text.strip()
        if is_old_testament(book_name):
            link = BASE_URL + "ot/" + a['href'].strip()
        else:
            link = BASE_URL + a['href'].strip()
        chapters[title] = link  # ç« ç¯€æ¨™é¡Œ: ç« ç¯€ç¶²å€
    return chapters

def scrape_chapter_content(chapter_url):
    """æ“·å–å–®ç¯‡ç« ç¯€çš„å…§å®¹"""
    response = requests.get(chapter_url)
    soup = BeautifulSoup(response.text, 'html.parser')

    content_div = soup.find('div', class_='ls-text')
    
    if not content_div:
        print(f"âŒ ç„¡æ³•æ‰¾åˆ°ä¸»æ–‡å…§å®¹", chapter_url)
        return None

    return content_div.get_text("\n", strip=True) if content_div else "å…§å®¹æŠ“å–å¤±æ•—"

def main():
    book_links = get_book_links()  # å–å¾—æ‰€æœ‰æ›¸å·
    all_books_content = {}

    for book_name, book_url in book_links.items():
        # **å…ˆæª¢æŸ¥ JSON æª”æ¡ˆæ˜¯å¦å·²å­˜åœ¨**
        safe_book_name = sanitize_filename(book_name)  # ç¢ºä¿æª”ååˆæ³•
        json_filename = f"{safe_book_name}.json"

        if os.path.exists(json_filename):
            print(f"â­ï¸  è·³éã€Š{book_name}ã€‹ï¼Œæª”æ¡ˆå·²å­˜åœ¨ ({json_filename})")
            continue  # **è·³éå·²å­˜åœ¨çš„æ›¸å·**


        print(f"ğŸ“– æŠ“å–ã€Š{book_name}ã€‹ç´¢å¼•...")
        chapter_links = get_chapter_links(book_name, book_url)  # å–å¾—æ›¸å·å…§æ‰€æœ‰ç« ç¯€
        book_content = {}

        for chapter_name, chapter_url in chapter_links.items():
            print(f"ğŸ“„ æŠ“å–ã€Š{book_name}ã€‹{chapter_name}...")
            book_content[chapter_name] = scrape_chapter_content(chapter_url)
            time.sleep(1)  # é¿å…éå¿«è«‹æ±‚è¢«å°é–

        all_books_content[book_name] = book_content  # å­˜å…¥è©²æ›¸å·çš„å…§å®¹

        with open(f"{sanitize_filename(book_name)}.json", "w", encoding="utf-8") as f:
            json.dump(book_content, f, ensure_ascii=False, indent=4)
        
        print(f"âœ… å·²å„²å­˜ {book_name}")

    # å„²å­˜ç‚º JSON æª”
    with open("ç”Ÿå‘½è®€ç¶“å®Œæ•´.json", "w", encoding="utf-8") as f:
        json.dump(all_books_content, f, ensure_ascii=False, indent=4)

    print("âœ… æ‰€æœ‰æ›¸å·èˆ‡ç« ç¯€æŠ“å–å®Œæˆï¼")

if __name__ == "__main__":
    main()
