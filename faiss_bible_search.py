import os
import json
import faiss
import numpy as np
from openai import OpenAI
from config import OPENAI_API_KEY  # åŒ¯å…¥ç’°å¢ƒè®Šæ•¸
from tqdm import tqdm  # é€²åº¦æ¢
import time

# è¼‰å…¥ OpenAI API
client = OpenAI(api_key=OPENAI_API_KEY)

# è¨­å®š JSON è³‡æ–™å¤¾èˆ‡ FAISS ç´¢å¼•æª”æ¡ˆ
JSON_FOLDER = "bible_json/å„ç« "
FAISS_INDEX_FILE = "bible_faiss-030301.index"
TEXTS_FILE = "bible_texts-030301.json"  # å­˜å„²æ–‡æœ¬è³‡æ–™

# è¨­å®šå‘é‡ç¶­åº¦ï¼ˆtext-embedding-ada-002 çš„ç¶­åº¦ç‚º 1536ï¼‰
dimension = 1536
# å‰µå»º IVFFlat ç´¢å¼•ï¼Œnlist æ˜¯ç°‡çš„æ•¸é‡ï¼Œé©åˆå¤§æ•¸æ“šé‡
# nlist = 610  # è¨­å®šå€’æ’æ¡¶æ•¸ï¼Œé€šå¸¸æ˜¯ sqrt(æ•¸æ“šé‡)
nlist = 2749  # è¨­å®šå€’æ’æ¡¶æ•¸ï¼Œé€šå¸¸æ˜¯ sqrt(æ•¸æ“šé‡)
quantizer = faiss.IndexFlatL2(dimension)  # é‡åŒ–å™¨
index = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_L2)  # IVFFlat ç´¢å¼•ï¼ˆéœ€è¨“ç·´ï¼‰


# å­˜å„²ç¶“æ–‡å‘é‡èˆ‡å°æ‡‰æ–‡æœ¬
texts = []

def train_faiss_index(embeddings):
    """è¨“ç·´ FAISS IVFFlat ç´¢å¼•ï¼Œä¸¦é¡¯ç¤ºé€²åº¦æ¢"""
    print("ğŸ”„ è¨“ç·´ FAISS IVFFlat ç´¢å¼•...")
    
    # è¨“ç·´éœ€è¦æ¨£æœ¬ï¼Œé€™è£¡ç”¨é€²åº¦æ¢æ¨¡æ“¬
    num_steps = 20  # è¨“ç·´é€²åº¦é¡¯ç¤º 10 æ­¥
    step_size = len(embeddings) // num_steps

    for i in tqdm(range(num_steps), desc="ğŸš€ FAISS è¨“ç·´ä¸­", unit="step"):
        time.sleep(0.5)  # æ¨¡æ“¬è¨“ç·´å»¶é²ï¼ˆå¯¦éš› train() æœƒåŒæ­¥å®Œæˆï¼‰
    
    index.train(embeddings)  # åŸ·è¡Œè¨“ç·´
    print("âœ… FAISS è¨“ç·´å®Œæˆï¼")

    return index  # å›å‚³å·²è¨“ç·´å¥½çš„ç´¢å¼•


# å–å¾—æ–‡å­—å‘é‡
def get_embedding(text):
    response = client.embeddings.create(input=text, model="text-embedding-ada-002")
    return np.array(response.data[0].embedding, dtype=np.float32)

def build_faiss_index():
    """å»ºç«‹ FAISS ç´¢å¼•"""
    global texts
    embeddings = []
    
    # éæ­· `bible_json/` ç›®éŒ„ï¼Œè®€å–æ‰€æœ‰ JSON æª”æ¡ˆ
    for filename in os.listdir(JSON_FOLDER):
        if filename.endswith(".json"):
            file_path = os.path.join(JSON_FOLDER, filename)
        
            # è®€å– JSON æª”æ¡ˆ
            with open(file_path, "r", encoding="utf-8") as f:
                bible_data = json.load(f)

            # è½‰æ›ç¶“æ–‡ç‚ºå‘é‡
            for verse in bible_data:
                book_chapter = filename.replace(".json", "")  # å–å¾—æ›¸å·èˆ‡ç« ç¯€åç¨±
                verse_text = f"{book_chapter} {verse['verse']} {verse['text']}"

                # åˆä½µè¨»è§£å…§å®¹
                notes_text = "".join([note['content'] for note in verse.get('notes', {}).values()])
                full_text = verse_text + " " + notes_text

                texts.append(full_text)

    # ä½¿ç”¨ tqdm ä¾†é¡¯ç¤ºé€²åº¦æ¢
    for text in tqdm(texts, desc="âœ¨ ç”ŸæˆåµŒå…¥å‘é‡"):
        vector = get_embedding(text)
        embeddings.append(vector)

    embeddings = np.array(embeddings, dtype=np.float32)

    # 1. **å…ˆè¨“ç·´ IVFFlat ç´¢å¼•**
    print("ğŸ”„ è¨“ç·´ FAISS IVFFlat ç´¢å¼•...")
    # è¨“ç·´ FAISS IVFFlat ç´¢å¼•
    index = train_faiss_index(embeddings)
    # index.train(embeddings)  # IVFFlat éœ€è¦å…ˆè¨“ç·´

    # 2. **å†åŠ å…¥å‘é‡**
    BATCH_SIZE = 10000  # æ ¹æ“š RAM è¨­å®šæ‰¹æ¬¡å¤§å°
    num_batches = len(embeddings) // BATCH_SIZE + (1 if len(embeddings) % BATCH_SIZE != 0 else 0)

    print("ğŸ”„ æ­£åœ¨å°‡å‘é‡åŠ å…¥ FAISS ç´¢å¼•...")

    for i in tqdm(range(num_batches), desc="ğŸ“¥ æ·»åŠ é€²åº¦", unit="batch"):
        start = i * BATCH_SIZE
        end = min((i + 1) * BATCH_SIZE, len(embeddings))
        index.add(embeddings[start:end])

    print("âœ… å‘é‡æ·»åŠ å®Œæˆï¼")

    # å„²å­˜ FAISS ç´¢å¼•

    # **æ¨¡æ“¬é€²åº¦æ¢**
    with tqdm(total=100, desc="ğŸ’¾ å„²å­˜é€²åº¦", unit="%") as pbar:
        for _ in range(10):  # æ¨¡æ“¬ 10 å€‹æ­¥é©Ÿ
            faiss.write_index(index, FAISS_INDEX_FILE)  # é€™è¡Œé‚„æ˜¯æœƒç›´æ¥å®Œæˆ
            time.sleep(0.2)  # æ¨¡æ“¬å¯«å…¥å»¶é²
            pbar.update(10)  # æ¯æ¬¡æ›´æ–° 10%

    # å„²å­˜æ–‡æœ¬è³‡æ–™
    with open(TEXTS_FILE, "w", encoding="utf-8") as f:
        json.dump(texts, f, ensure_ascii=False, indent=4)

    print(f"âœ… FAISS ç´¢å¼•å·²å»ºç«‹ä¸¦å„²å­˜ç‚º `{FAISS_INDEX_FILE}`")

def load_faiss_index():
    """è¼‰å…¥ FAISS ç´¢å¼•"""
    global texts
    if os.path.exists(FAISS_INDEX_FILE) and os.path.exists(TEXTS_FILE):
        faiss.read_index(FAISS_INDEX_FILE)
        with open(TEXTS_FILE, "r", encoding="utf-8") as f:
            texts = json.load(f)
        print("âœ… å·²è¼‰å…¥ FAISS ç´¢å¼•èˆ‡ç¶“æ–‡æ–‡æœ¬è³‡æ–™")
    else:
        print("âš ï¸ æœªæ‰¾åˆ° FAISS ç´¢å¼•ï¼Œå°‡é‡æ–°å»ºç«‹...")
        build_faiss_index()

# å…ˆå˜—è©¦è¼‰å…¥ç´¢å¼•ï¼Œè‹¥ä¸å­˜åœ¨å‰‡å»ºç«‹
load_faiss_index()

# æŸ¥è©¢å‡½å¼
def search_bible(query, top_k=3):
    """æœå°‹æœ€ç›¸é—œçš„ç¶“æ–‡"""
    query_vector = get_embedding(query).reshape(1, -1)
    distances, indices = index.search(query_vector, top_k)

    # å–å¾—æœ€ç›¸é—œçš„ç¶“æ–‡
    retrieved_texts = [texts[i] for i in indices[0]]

    # çµ„åˆç¶“æ–‡ä½œç‚ºä¸Šä¸‹æ–‡
    context = "\n".join(retrieved_texts)
        
    # å‚³å…¥ OpenAI API é€²è¡Œå›ç­”
    prompt = f"æ ¹æ“šä»¥ä¸‹è–ç¶“ç¶“æ–‡å›ç­”å•é¡Œï¼š\n{context}\n\nå•é¡Œï¼š{query}\nå›ç­”ï¼š"

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "system", "content": "ä½ æ˜¯ä¸€ä½æœ‰è¶£çš„èŠå¤©å“¡ï¼Œæ“…é•·é—œå¿ƒé–‹å•Ÿè©±é¡Œï¼Œä¸¦å¸¸åˆ†äº«æ¢å¾©æœ¬è–ç¶“çš„ç¶“æ–‡ã€‚"},
                  {"role": "user", "content": prompt}]
    )
    
    return response.choices[0].message.content.strip()

# æ¸¬è©¦æŸ¥è©¢
# query = "å¾ä¿ç¾…çš„æ›¸ä¿¡ä¸­ï¼Œå¦‚ä½•å•Ÿç¤ºå‡ºç¥çš„ç¶“ç¶¸ï¼Ÿ"
# print(search_bible(query))
